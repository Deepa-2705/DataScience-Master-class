{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7071257e-b937-4f15-ba87-89816b3f4051",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\n",
    "Ans: Simple linear regression is a linear approach to model the relationship between a dependent variable and one independent variable. \n",
    "\n",
    "Ex:finding a relationship between the revenue and temperature, with a sample size for revenue as the dependent variable. \n",
    "\n",
    "Multiple linear regression uses a linear function to predict the value of a dependent variable containing the function n independent variables.\n",
    "\n",
    "Example: Prediction of CO2 emission based on engine size and number of cylinders in a car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b62c2-c8c8-4da0-88df-ee92d7c1ce9a",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "Ans:  Here are the key assumptions of linear regression and ways to check them:\n",
    "\n",
    "1) Linearity Assumption:\n",
    "\n",
    "Assumption: The relationship between the independent variables and the dependent variable is linear.\n",
    "\n",
    "Check: You can visually inspect scatterplots of the dependent variable against each independent variable. If the data points appear to follow a linear pattern, the assumption may hold. Additionally, you can use residual plots to assess linearity after fitting the regression model.\n",
    "\n",
    "2) Independence Assumption:\n",
    "\n",
    "Assumption: The residuals (the differences between observed and predicted values) are independent of each other.\n",
    "\n",
    "Check: Plot a scatterplot of residuals against the order of observation (time or data collection order). Look for patterns, trends, or correlations in the residuals. If no clear pattern is visible, the assumption may hold.\n",
    "\n",
    "3) Homoscedasticity Assumption:\n",
    "\n",
    "Assumption: The variance of the residuals is constant across all levels of the independent variables (homoscedasticity).\n",
    "\n",
    "Check: Create a scatterplot of residuals against the predicted values or the independent variables. If the spread of residuals appears fairly consistent, the assumption may hold. Alternatively, you can perform statistical tests like the Breusch-Pagan test or the White test for heteroscedasticity.\n",
    "\n",
    "4) Normality of Residuals Assumption:\n",
    "\n",
    "Assumption: The residuals are normally distributed.\n",
    "\n",
    "Check: Create a histogram or a Q-Q plot of the residuals and visually assess whether they approximate a normal distribution. You can also use statistical tests like the Shapiro-Wilk test or the Anderson-Darling test for normality.\n",
    "\n",
    "5) No or Little Multicollinearity Assumption:\n",
    "\n",
    "Assumption: The independent variables are not highly correlated with each other (no multicollinearity).\n",
    "\n",
    "Check: Calculate correlation coefficients (e.g., Pearson's correlation) between pairs of independent variables. High correlation coefficients indicate potential multicollinearity. Additionally, calculate the Variance Inflation Factor (VIF) for each independent variable; high VIF values (> 10) may indicate multicollinearity.\n",
    "\n",
    "6) No or Little Endogeneity Assumption:\n",
    "\n",
    "Assumption: The independent variables are not influenced by the error term in the regression model.\n",
    "\n",
    "Check: Assess whether there are theoretical reasons or prior knowledge suggesting endogeneity. Addressing endogeneity often requires advanced techniques such as instrumental variables or control function approaches.\n",
    "\n",
    "7) No or Little Autocorrelation Assumption:\n",
    "\n",
    "Assumption: The residuals are not correlated with each other (no autocorrelation).\n",
    "\n",
    "Check: For time-series data, you can use autocorrelation plots (ACF and PACF plots) or statistical tests like the Durbin-Watson test to check for autocorrelation in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776c241-d51c-4c98-9630-c4e45eb0eda2",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "Ans: \n",
    "\n",
    "* Slope (Coefficient of the Independent Variable):\n",
    "\n",
    "1) The slope represents the change in the dependent variable for a one-unit change in the independent variable, while holding all other variables constant.\n",
    "2) It indicates the strength and direction of the linear relationship between the variables. A positive slope means that as the independent variable increases, the dependent variable tends to increase as well, and a negative slope indicates the opposite.\n",
    "3) The magnitude of the slope represents the rate of change; larger values indicate a steeper slope, implying a stronger relationship.\n",
    "\n",
    "* Intercept:\n",
    "\n",
    "1) The intercept represents the predicted value of the dependent variable when all independent variables are set to zero.\n",
    "2) In many real-world cases, the intercept may not have a meaningful interpretation, especially if zero values for all independent variables are not possible or relevant. It mainly serves to ensure that the regression line fits the data.\n",
    "\n",
    "\n",
    "Example: Predicting House Prices\n",
    "\n",
    "Suppose you are analyzing a dataset of house prices, and you want to understand how the size (in square feet) of a house (independent variable) affects its price (dependent variable).\n",
    "\n",
    "Slope Interpretation: Let's say the slope in your linear regression model is 100. This means that, on average, for every additional square foot of living space in a house, the price of the house tends to increase by  doller 100, assuming  all  other factors (e.g., location, number of bedrooms, condition) remain constant. If the slope were -100, it would imply that, on average, for every additional square foot, the price tends to decrease by $100.\n",
    "\n",
    "Intercept Interpretation: If the intercept in your model is $50,000, it represents the estimated price of a house with zero square feet of living space. This intercept might not have a practical meaning since houses cannot have zero square footage, but it is included to help the regression line fit the data.\n",
    "\n",
    "Putting it all together, in this scenario, the linear regression model might look something like this:\n",
    "\n",
    "Price = 100 * Size (in square feet) + 50,000\n",
    "\n",
    "So, if you have a house with 2,000 square feet, the predicted price would be:\n",
    "\n",
    "Price = 100 * 2,000 + 50,000 = $250,000\n",
    "\n",
    "Keep in mind that linear regression models are simplifications of real-world relationships and may not capture all nuances. Interpretations should be made with caution and consider the context of the data and the limitations of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772dc4ec-0f67-4a5d-8488-460b332b67bd",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Ans: Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.\n",
    "\n",
    "The main aim of gradient descent is to find the best parameters of a model which gives the highest accuracy on training as well as testing datasets. In gradient descent, The gradient is a vector that points in the direction of the steepest increase of the function at a specific point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fccac9-2c6f-4819-bfe4-66fad22dedc0",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Ans: Multiple linear regression is a regression model that estimates the relationship between a quantitative dependent variable and two or more independent variables using a straight line\n",
    "\n",
    "Multiple regression differs from simple linear regression because it: uses more than one independent variable to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1df75-b075-48c0-8c46-2940bb7a3c82",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "Ans: Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n",
    "\n",
    "Multicollinearity exists whenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation. Multicollinearity is a problem because it will make the statistical inferences less reliable.\n",
    "\n",
    "One method to detect multicollinearity is to calculate the variance inflation factor (VIF) for each independent variable, and a VIF value greater than 1.5 indicates multicollinearity.\n",
    "\n",
    "One way to address multicollinearity is to center the predictors, that is substract the mean of one series from each value. Ridge regression can also be used when data is highly collinear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15780f-4fe7-49d6-a631-c0fa8e6d460b",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Ans: A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship.\n",
    "\n",
    "Polynomial Regression is able to model non-linearly seperable data and is much more flexible than linear regression, but some of its disadvantages are we need some knowledge of data in order to select best exponents, and it is prone to over fitting if exponents are poorly selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a34ec-88c0-40a6-9181-ecb1e50bb522",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Ans: Advantages:\n",
    "    \n",
    "1) Non-linear relationships: Polynomial regression can capture non-linear relationships between variables, unlike linear regression, which assumes a linear relationship. This flexibility is helpful when the underlying relationship is curvilinear.\n",
    "2) Improved fit: By allowing for more flexible curve fitting, polynomial regression can often provide a better fit to the data compared to linear regression. This can help in finding the best-fitting curve among different options.\n",
    "3) Local approximation: Polynomial regression can approximate the data in a specific region more accurately by adjusting the degree of the polynomial. This enables a better fit within localized areas of the dataset.\n",
    "4) Versatility: Polynomial regression can handle a wide range of functions by adjusting the degree of the polynomial. It allows for modeling complex relationships without needing to use more advanced techniques.\n",
    "5) Interactions: Polynomial regression can account for interactions between different predictor variables by including interaction terms in the polynomial equation. This allows for better understanding and modeling of complex interaction effects.\n",
    "\n",
    "Disadvantage:\n",
    "\n",
    "1) One or two outliers in the data might have a significant impact on the nonlinear analysisâ€™ outcomes. These are overly reliant on outliers. \n",
    "2) Furthermore, there are fewer model validation methods for detecting outliers in nonlinear regression than there are for linear regression.\n",
    "\n",
    "The relationship of the independent and dependent variable on a graph turns out as a curvilinear relationship with the help of a polynomial equation. Polynomial regression is used when there is no linear correlation between the variables. Hence, it explains why it looks more like a non-linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ffa6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
